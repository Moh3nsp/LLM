# 🔥 Token to Transformer — An End-to-End Guide to Building Large Language Models

Welcome to **LLM From Scratch**, a comprehensive and educational implementation of a complete Large Language Model pipeline — from tokenization to fine-tuning. This repository is designed to **demystify the inner workings of LLMs** like GPT by offering a clean and modular source code that covers **every major component** of modern transformer-based models.

## 🌟 Why This Repository?

Building a working LLM requires understanding several deeply interconnected concepts. This repo is not just code — it’s a learning journey.

✔️ Ideal for **researchers**, **ML engineers**, **students**, and **enthusiasts**  
✔️ Focused on **clarity**, **modularity**, and **educational value**  
✔️ Covers the **entire LLM lifecycle**, from raw text to a fine-tuned model
 

## 🧠 What You’ll Learn

This project includes detailed implementations of:

### 🔡 Tokenizer
- Subword tokenization logic
- Custom vocabulary building
- Encoding and decoding utilities

### 🧠 Attention Mechanism
- Scaled dot-product attention
- Multi-head self-attention
- Masking and positional bias

### 🏗️ Transformer Architecture
- Encoder/Decoder blocks
- Layer normalization & residuals
- Feedforward layers

### 🤖 GPT Architecture
- Decoder-only transformer
- Autoregressive generation
- Causal masking

### 📚 Pre-training
- Language modeling objective (e.g., next token prediction)
- Training loop with dataset loading
- Optimizers and scheduling

### 🛠️ Fine-tuning
- Adapting the pre-trained model for downstream tasks
- Transfer learning workflows
- Evaluation metrics
 
 ## 📚 Resources & Inspirations

This project was built by learning from the best materials in the field. Key inspirations include:


- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT Architecture Papers (GPT-1, GPT-2, GPT-3)](https://openai.com/research) by OpenAI   
- [Hands-On Large Language Models](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)  
- [LLM Engineer's Handbook](https://www.oreilly.com/library/view/llm-engineers-handbook/9781836200079/)  

- Deep Learning Specialization — Coursera (Andrew Ng)  
- PyTorch official tutorials and documentation  
- Blogs, tutorials, and YouTube channels explaining LLMs and Transformers

> If you’re one of the authors above: Thank you for sharing your knowledge! 🙏